{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from linkedin_api import Linkedin\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsRelatedToSponsorship = [None, \"sponsorship\", \"visa\", \"right to work\", \"right to stay\", \"h1b\", \"h-1b\", \"authorized to work\", \"work authorization\", \"sponsor\", \"sponsored\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.client import RemoteDisconnected\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "api = Linkedin('georgerithika7@gmail.com', 'RandomPassword2020!')\n",
    "\n",
    "def getJobsFromLinkedIn(searchKeyword):\n",
    "    # Fetch job search results\n",
    "    print(\"Fetching Job\")\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        print(attempt)\n",
    "        try:\n",
    "            with tqdm(total=1, desc=\"Retrieving Jobs\", unit=\"job\") as pbar:\n",
    "                response = []\n",
    "                if(searchKeyword is None): response = api.search_jobs()\n",
    "                else: response = api.search_jobs(searchKeyword)\n",
    "                pbar.update(1)\n",
    "            print(\"Retrieved Jobs\")\n",
    "\n",
    "            # Extract job IDs from 'trackingUrn'\n",
    "            job_ids = [\n",
    "                job.get('trackingUrn', '').split(':')[-1]\n",
    "                for job in response\n",
    "                if job.get('trackingUrn', '').startswith('urn:li:jobPosting:')\n",
    "            ]\n",
    "            print(\"Mapped to IDs\")\n",
    "            \n",
    "            with tqdm(total=len(job_ids), desc=\"Fetching Job Details\", unit=\"job\") as pbar:\n",
    "                # Use ThreadPoolExecutor to fetch job details in parallel\n",
    "                with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust 'max_workers' based on performance\n",
    "                    # Define a wrapper function to update progress\n",
    "                    def fetch_and_update(job_id):\n",
    "                        job_detail = getJobDetailFromLinkedIn(job_id)\n",
    "                        pbar.update(1)  # Update progress bar after each job detail is fetched\n",
    "                        return job_detail\n",
    "\n",
    "                    job_details_list = list(executor.map(fetch_and_update, job_ids))\n",
    "        \n",
    "            print(\"Fetching of Jobs Completed\")\n",
    "            return job_details_list\n",
    "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, RemoteDisconnected) as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)  # Exponential backoff with jitter\n",
    "                print(f\"Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch joblist after {max_retries} attempts.\")\n",
    "                return None\n",
    "\n",
    "    \n",
    "    \n",
    "# Define a function to fetch job details for a given job ID\n",
    "def getJobDetailFromLinkedIn(job_id):\n",
    "\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "\n",
    "            job_data = api.get_job(job_id)\n",
    "\n",
    "            # Only return specific fields that you need (e.g., title, company, location)\n",
    "            filtered_job_data = {\n",
    "                'job_posting_id': job_data.get('jobPostingId', 'N/A'),\n",
    "                'title': job_data.get('title', 'N/A'),\n",
    "                'company': job_data.get('companyDetails', {}).get('com.linkedin.voyager.deco.jobs.web.shared.WebCompactJobPostingCompany', {}).get('companyResolutionResult', {}).get('name', 'N/A'),\n",
    "                'location': job_data.get('formattedLocation', 'N/A'),\n",
    "                'description': job_data.get('description', {}).get('text', 'N/A'),\n",
    "                'job_site': \"LinkedIn\"\n",
    "            }\n",
    "            return filtered_job_data\n",
    "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, RemoteDisconnected) as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)  # Exponential backoff with jitter\n",
    "                print(f\"Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch details for job {job_id} after {max_retries} attempts.\")\n",
    "                return None\n",
    "\n",
    "\n",
    "def getJobsForLinkedInAndSaveToCsvFile(keyword):\n",
    "    linkedInData = getJobsFromLinkedIn(None)\n",
    "    df = pd.DataFrame(linkedInData)\n",
    "    print(\"Moving to csv file\")\n",
    "\n",
    "    with tqdm(total=len(linkedInData), desc=\"Moving to csv file\", unit=\"job\"):\n",
    "        df.to_csv(\"linkedin_data.csv\", mode=\"a\", header=(keyword is None), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authorized to work\n",
      "work authorization\n",
      "sponsor\n",
      "sponsored\n",
      "Completed exporting Data from LinkedIn\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    for keyword in keywordsRelatedToSponsorship[7:]:\n",
    "        future = executor.submit(getJobsForLinkedInAndSaveToCsvFile, keyword)\n",
    "        future.result()\n",
    "    print(\"Completed exporting Data from LinkedIn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDataToDB(data):\n",
    "    conn = sqlite3.connect('labelled_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "                   CREATE TABLE IF NOT EXISTS labelled_job_postings(\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    job_posting_id TEXT,\n",
    "                    title TEXT,\n",
    "                    company TEXT,\n",
    "                    location TEXT,\n",
    "                    description TEXT,\n",
    "                    job_site TEXT,\n",
    "                    sponsorship_available TEXT\n",
    "                   )\n",
    "                   \n",
    "                   '''\n",
    "    )\n",
    "    conn.commit()\n",
    "    cursor.executemany('''\n",
    "                        INSERT INTO labelled_job_postings(job_posting_id, title, company, location, description, job_site)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    ''', data.values.tolist()\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData():\n",
    "    df = pd.read_csv(\"linkedin_data.csv\")\n",
    "    df_cleaned = df.drop_duplicates(subset=\"job_posting_id\", keep=\"first\")\n",
    "    df_cleaned = df.dropna()\n",
    "    addDataToDB(df_cleaned)\n",
    "    # df_cleaned.to_csv('linkedin_data_cleaned.csv', index=False)\n",
    "cleanData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18916\n"
     ]
    }
   ],
   "source": [
    "def readDbData():\n",
    "    conn = sqlite3.connect('labelled_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "                   SELECT count(*) FROM  labelled_job_postings\n",
    "                   \n",
    "                   '''\n",
    "    )\n",
    "    rowcount = cursor.fetchone()[0]\n",
    "    print(rowcount)\n",
    "    conn.close()\n",
    "\n",
    "readDbData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
